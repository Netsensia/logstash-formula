{% from "logstash/map.jinja" import logstash with context %}

# based on: http://cookbook.logstash.net/recipes/syslog-pri/
input {
    tcp {
        port => 2514
        type => syslog
    }
    udp {
        port => 2514
        type => syslog
    }
    redis {
        host => '{{logstash.redis}}'
        data_type => list
        key => 'logstash:beaver'
        type => 'logstash:beaver'
    }
}


filter {
  if [type] == "syslog" {
    # syslog preprocessing
    # it will replace message and host with syslog entry and parse syslog header
    grok {
      match => {
        "message" => "<%{POSINT:syslog_pri}>%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:host} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:message}"
      }
      overwrite => [ "message", "host" ]
      add_field => [ "received_at", "%{@timestamp}" ]
      add_field => [ "received_from", "%{host}" ]
    }

    # Officially it adds syslog_pri field (already in place).
    # More importantly it adds:
    # TODO: list fields
    syslog_pri { }

    # Let's use syslog server timestamp as an ordering timestamp (looks like a workaround for some old logstash bug)
    date {
      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }

    # all syslog parsers in one place
    grok {
      match => [
        "message", "IPTables-Dropped: %{IPTABLES}",
        "message", "%{HAPROXYHTTP}",
        "message", "%{HAPROXYTCP}"
      ]
      patterns_dir => ["/etc/logstash/patterns"]
    }

    # lets split haproxy key=value pairs
    # use haproxy timestamp to sort events
    # tidy up kibana table view
    if [syslog_program] == "haproxy" {
      kv {
          source => "keyvalue"
          remove_field => "keyvalue"
      }
      date {
        match => {
            "accept_date" => "dd/MMM/YYYY:HH:mm:ss.SSS"
        }
        remove_field => "haproxy_hour"
        remove_field => "haproxy_minute"
        remove_field => "haproxy_second"
        remove_field => "haproxy_milliseconds"
        remove_field => "haproxy_monthday"
        remove_field => "haproxy_month"
        remove_field => "haproxy_year"
        remove_field => "haproxy_time"
      }
    }
  }

  if 'audit' in [tags] {
    grok {
      patterns_dir => "/etc/logstash/patterns"
      match => [
        "message", "%{AUDIT}",
        "message", "%{AUDITLOGIN}",
        "message", "%{AUDITOTHER}"
      ]
    }
  }

  if 'kernel' in [tags] {
    grok {
      patterns_dir => ["/etc/logstash/patterns"]
      match => [ "message", "%{APPARMOR}" ]
    }
    if '_grokparsefailure' not in [tags] {
      mutate {
        replace => ["tags", "apparmor"]
      }
    }
  }

  # TODO: consider flattening nginx logs so that we don't have @fields.xxx and i.e. we can easily filter logs
  # on request_id combining them from all different sources

  # following flattens just request_id (handy for events grouping in kibana)
  if [@fields] and [@fields][request_id] {
      mutate {
        replace => [ "request_id", "%{[@fields][request_id]}" ]
      }
  }
}


output {
    elasticsearch_http {
        host => "{{logstash.elasticsearch.host}}"
        port => "{{logstash.elasticsearch.port}}"
    }
}
